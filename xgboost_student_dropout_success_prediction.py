# -*- coding: utf-8 -*-
"""XGBoost_student_dropout_success_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kBxxhSs2EuqQd8362QXzM1wQWyD6NGyw

# **LOAD DATA**
"""

from google.colab import files
files.upload()  # Upload kaggle.json di sini

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d adilshamim8/predict-students-dropout-and-academic-success

# Unzip file
!unzip predict-students-dropout-and-academic-success.zip

import pandas as pd

# Cek nama file .csv-nya dulu, kemungkinan besar:
df = pd.read_csv("/content/students_dropout_academic_success.csv")  # Ganti dengan nama file sebenarnya jika berbeda

"""# **IMPORT LIBRARY**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

"""# **DATA EXPLORATORY**"""

df

df.head()

print(df.columns)

df.info()

df.describe()

numerical_features = [
    'Marital Status',
    'Application mode',
    'Application order',
    'Course',
    'Daytime/evening attendance',
    'Previous qualification',
    'Previous qualification (grade)',
    'Nacionality',
    "Mother's qualification",
    "Father's qualification",
    "Mother's occupation",
    "Father's occupation",
    "Admission grade",
    'Displaced',
    'Educational special needs',
    'Debtor',
    'Tuition fees up to date',
    'Gender',
    'Scholarship holder',
    'Age at enrollment',
    'International',
    'Curricular units 1st sem (credited)',
    'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)',
    'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)',
    'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)',
    'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)',
    'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)',
    'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate',
    'Inflation rate',
    'GDP'
]
plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Sebaran data berdasarkan kolom Target"""

# Target distribution
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df, x='target', palette='viridis')
plt.title('Student Outcomes Distribution', fontsize=16)
plt.xlabel('Outcome', fontsize=14)
plt.ylabel('Number of Students', fontsize=14)

"""# **DATA PREPROCESSING**

Mengecek apakah ada nilai yang kosong
"""

# Melihat apakah ada missing values
missing_values = df.isnull().sum()
print("\nMissing values per column:")
print(missing_values[missing_values > 0]) if any(missing_values > 0) else print("No missing values found.")

"""Mengecek apakah ada nilai yang duplikat"""

# Melihat apakah ada duplikasi data
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")

"""Check outliers pada data dengan continous value"""

# Pilih kolom float saja
continuous = df.select_dtypes(include='float64').columns.tolist()

# Atau: kolom int dengan >10 unique values
int_cols = df.select_dtypes(include='int64').nunique()
cont_int = int_cols[int_cols > 10].index.tolist()

numeric_continuous = continuous + cont_int
for col in numeric_continuous:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]
    outlier_pct = outliers_count / len(df) * 100
    print(f"{col}: {outliers_count} outliers ({outlier_pct:.2f}%)")

"""Normalisasi outlier dengan menggunakan IQR dan menentukan batas bawah dan batas atasnya"""

# Step 3: Cap outliers for key columns
print("Memperbaiki nilai outliers untuk kolom continous")
for col in numeric_continuous:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Don't go below 0 for grades
    upper_bound = Q3 + 1.5 * IQR

    # Cap outliers instead of removing them
    df[col] = df[col].clip(lower_bound, upper_bound)

    print(f"Column {col} capped between {lower_bound:.2f} and {upper_bound:.2f}")

"""Memisahkan fitur (kolom) numerik dan kategorikal dalam sebuah DataFrame (df) agar bisa dilakukan pembersihan atau pemrosesan data secara spesifik sesuai tipe datanya."""

# === 1. Pisahkan fitur numerik dan kategorikal ===
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

"""Membersihkan data kategorikal dengan cara menghapus atau mengganti kategori yang jarang muncul (kurang dari 1%) di setiap kolom kategorikal."""

# === 2. Kategorikal Cleaning: hapus kategori langka (< 1%) ===
def remove_rare_categories(df, col, threshold=0.01):
    value_counts = df[col].value_counts(normalize=True)
    common_categories = value_counts[value_counts > threshold].index
    df[col] = df[col].apply(lambda x: x if x in common_categories else 'Other')
    return df

for col in categorical_cols:
    df = remove_rare_categories(df, col)

"""Menormalkan atau men-standarisasi kolom numerik dengan StandardScaler, yaitu mengubah data agar memiliki rata-rata 0 dan standar deviasi 1."""

# === 4. Scaling / Normalization ===
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Prepare features and target
X = df.drop('target', axis=1)

# Encode target
le = LabelEncoder()
y = le.fit_transform(df['target'])
print(f"Target classes: {dict(zip(le.classes_, range(len(le.classes_))))}")

"""SMOTE untuk mengatasi ketidakseimbangan (imbalance) kelas dalam dataset klasifikasi."""

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

"""# **MEMBANGUN MODEL**"""

# Gunakan XGBClassifier
model = XGBClassifier()
model.fit(X_train_res, y_train_res)

# Prediksi dan evaluasi
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# **KESIMPULAN**"""

# Feature importance
feat_importances = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\n Top 5 factors predicting student outcomes:")
for i, (feature, importance) in enumerate(zip(feat_importances['Feature'].head(5), feat_importances['Importance'].head(5))):
    print(f"{i+1}. {feature}: {importance:.4f}")

# Visualisasi top 15 fitur
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feat_importances.head(15))
plt.title('Top 15 Factors Influencing Student Success', fontsize=16)
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.tight_layout()
plt.show()

"""# **PERBANDINGAN DENGAN MODEL LAIN**"""

models = {
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "KNN": KNeighborsClassifier(),
}

# List for storing accuracy and classification report
accuracies = []
reports = []

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train_res, y_train_res)  # Train model
    y_pred = model.predict(X_test)  # Predict on test set
    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
    accuracies.append(acc)
    reports.append(classification_report(y_test, y_pred, output_dict=True))  # Store detailed report

# Plotting the accuracy for each model
plt.figure(figsize=(10, 6))
plt.bar(models.keys(), accuracies, color=['blue', 'green', 'red', 'orange'])
plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.show()

# Display classification reports for each model
for name, report in zip(models.keys(), reports):
    print(f"\n{name} Classification Report:")
    print("--------------------------------------------------")
    print(report)