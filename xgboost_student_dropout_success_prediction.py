# -*- coding: utf-8 -*-
"""XGBoost_student_dropout_success_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kBxxhSs2EuqQd8362QXzM1wQWyD6NGyw

# **LOAD DATA**

Mengimpor modul files dari google.colab dan menjalankan fungsi files.upload() untuk memungkinkan pengguna mengunggah file lokal (misalnya, kaggle.json) ke lingkungan Google Colab.
"""

from google.colab import files
files.upload()  # Upload kaggle.json di sini

"""Membuat direktori untuk menyimpan file konfigurasi API Kaggle, menyalin kredensial API, mengunduh dataset "predict-students-dropout-and-academic-success" dari Kaggle, dan mengekstraknya untuk digunakan dalam analisis."""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d adilshamim8/predict-students-dropout-and-academic-success

# Unzip file
!unzip predict-students-dropout-and-academic-success.zip

"""# **IMPORT LIBRARY**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

"""# **DATA UNDERSTANDING**

Memuat dataset dari file CSV
"""

df = pd.read_csv("/content/students_dropout_academic_success.csv")

"""Menampilkan preview dataset, dari kode ini sekilas kita bisa melihat bahwa ada 4424 baris dan 37 kolom."""

df

"""Menampilkan nama-nama kolom dalam DataFrame df."""

print(df.columns)

"""Menampilkan informasi tentang DataFrame df, termasuk tipe data setiap kolom dan jumlah data non-null. Dari kode ini kita bisa melihat bahwa semua kolom bertipe data numerik (int/float) dan hanya kolom target yang bertipe data object"""

df.info()

"""Menampilkan statistik deskriptif dari kolom-kolom numerik dalam DataFrame df, seperti mean, standar deviasi, nilai minimum, dan nilai maksimum."""

df.describe()

"""Membuat heatmap matriks korelasi untuk fitur numerik. Ini membantu dalam memahami hubungan antar fitur numerik dalam dataset."""

numerical_features = [
    'Marital Status',
    'Application mode',
    'Application order',
    'Course',
    'Daytime/evening attendance',
    'Previous qualification',
    'Previous qualification (grade)',
    'Nacionality',
    "Mother's qualification",
    "Father's qualification",
    "Mother's occupation",
    "Father's occupation",
    "Admission grade",
    'Displaced',
    'Educational special needs',
    'Debtor',
    'Tuition fees up to date',
    'Gender',
    'Scholarship holder',
    'Age at enrollment',
    'International',
    'Curricular units 1st sem (credited)',
    'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)',
    'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)',
    'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)',
    'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)',
    'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)',
    'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate',
    'Inflation rate',
    'GDP'
]
plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Membuat histogram untuk setiap kolom numerik dalam DataFrame df. Histogram ini memberikan gambaran tentang distribusi data untuk setiap fitur."""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Membuat count plot untuk kolom 'target'. Ini menunjukkan distribusi kelas target (Dropout, Enrolled, Graduate) dalam dataset. Terlihat ada pebedaan  jumlah yang sangat signifikan dari ketiga kelas, hal ini nantinya akan dilakukan penyeimbangan kelas dengan teknik SMOTE"""

# Target distribution
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df, x='target', palette='viridis')
plt.title('Student Outcomes Distribution', fontsize=16)
plt.xlabel('Outcome', fontsize=14)
plt.ylabel('Number of Students', fontsize=14)

"""Memeriksa dan menampilkan jumlah missing values (nilai yang hilang) di setiap kolom DataFrame df. Karena tidak ada missing values maka kita biarkan saja seperti ini."""

# Melihat apakah ada missing values
missing_values = df.isnull().sum()
print("\nMissing values per column:")
print(missing_values[missing_values > 0]) if any(missing_values > 0) else print("No missing values found.")

"""Memeriksa dan menampilkan jumlah baris duplikat dalam DataFrame df. Karena tidak ada baris duplikat maka kita biarkan saja seperti ini"""

# Melihat apakah ada duplikasi data
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")

"""Mendeteksi dan menampilkan outlier (nilai ekstrem) dalam kolom-kolom numerik kontinu menggunakan metode Interquartile Range (IQR). Output menunjukkan jumlah dan persentase data outlier dalam berbagai variabel akademik dan demografis, dengan proporsi outlier tertinggi pada variabel seperti "Curricular units 1st sem (grade)" dan "Curricular units 2nd sem (grade)", serta sejumlah variabel tanpa outlier, seperti tingkat pengangguran dan inflasi."""

# Pilih kolom float saja
continuous = df.select_dtypes(include='float64').columns.tolist()

# Atau: kolom int dengan >10 unique values
int_cols = df.select_dtypes(include='int64').nunique()
cont_int = int_cols[int_cols > 10].index.tolist()

numeric_continuous = continuous + cont_int
for col in numeric_continuous:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]
    outlier_pct = outliers_count / len(df) * 100
    print(f"{col}: {outliers_count} outliers ({outlier_pct:.2f}%)")

"""# **DATA PREPARATION**

Melakukan penanganan outlier pada kolom-kolom numerik kontinu yang telah dievaluasi sebelumnya dengan menghitung batas bawah dan atas menggunakan metode Interquartile Range (IQR), lalu membatasi nilai-nilai ekstrem (outlier) dalam rentang tersebut menggunakan clip(), sehingga nilai yang terlalu rendah atau terlalu tinggi dikonversi ke batas bawah atau atas, menjaga distribusi data tanpa membuang sampel.
"""

# Cap outliers for key columns
print("Memperbaiki nilai outliers untuk kolom continous")
for col in numeric_continuous:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Don't go below 0 for grades
    upper_bound = Q3 + 1.5 * IQR

    # Cap outliers instead of removing them
    df[col] = df[col].clip(lower_bound, upper_bound)

    print(f"Column {col} capped between {lower_bound:.2f} and {upper_bound:.2f}")

"""Memisahkan fitur (kolom) numerik dan kategorikal dalam sebuah DataFrame (df) agar bisa dilakukan pembersihan atau pemrosesan data secara spesifik sesuai tipe datanya."""

# Pisahkan fitur numerik dan kategorikal
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

"""Membersihkan kolom-kolom kategorikal dalam DataFrame dengan mengidentifikasi dan mengganti kategori langka—yaitu kategori dengan proporsi kemunculan kurang dari 1%—menjadi label 'Other', menggunakan fungsi remove_rare_categories() yang diterapkan ke setiap kolom dalam daftar categorical_cols"""

# Kategorikal Cleaning: hapus kategori langka (< 1%)
def remove_rare_categories(df, col, threshold=0.01):
    value_counts = df[col].value_counts(normalize=True)
    common_categories = value_counts[value_counts > threshold].index
    df[col] = df[col].apply(lambda x: x if x in common_categories else 'Other')
    return df

for col in categorical_cols:
    df = remove_rare_categories(df, col)

"""Melakukan normalisasi (scaling) terhadap kolom-kolom numerik dalam DataFrame menggunakan StandardScaler, yang mengubah distribusi data sehingga setiap fitur memiliki nilai rata-rata 0 dan standar deviasi 1"""

# Scaling / Normalization
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

"""Mempersiapkan fitur (X) dengan menghapus kolom 'target' dari DataFrame, lalu mengenali dan mengubah nilai kategorikal pada kolom 'target' menjadi angka menggunakan LabelEncoder

Target classes: {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}
"""

# Prepare features and target
X = df.drop('target', axis=1)

# Encode target
le = LabelEncoder()
y = le.fit_transform(df['target'])
print(f"Target classes: {dict(zip(le.classes_, range(len(le.classes_))))}")

"""Membagi dataset menjadi data pelatihan (X_train, y_train) dan data pengujian (X_test, y_test) dengan proporsi 90% untuk pelatihan dan 10% untuk pengujian, menggunakan train_test_split dengan random_state=42 untuk reprodusibilitas dan stratify=y agar distribusi kelas pada data target tetap seimbang di kedua subset"""

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

"""Menerapkan teknik SMOTE (Synthetic Minority Over-sampling Technique) pada data pelatihan untuk mengatasi ketidakseimbangan kelas dengan menghasilkan sampel sintetis pada kelas minoritas, sehingga X_train_res dan y_train_res menjadi data pelatihan yang telah seimbang dari segi distribusi kelas."""

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

"""# **MODEL DEVELOPMENT**

Membuat model XGBClassifier dengan menggunakan parameter default.
"""

# Gunakan XGBClassifier
model = XGBClassifier()
model.fit(X_train_res, y_train_res)

# Prediksi dan evaluasi
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# **INSIGHT**

Mengekstrak dan menampilkan apa saja fitur-fitur penting yang paling berpengaruh dalam hasil akhir mahasiswa
"""

# Feature importance
feat_importances = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

"""Model XGBClassifier menunjukkan bahwa 5 faktor paling berpengaruh dalam memprediksi hasil mahasiswa adalah jumlah mata kuliah yang lulus di semester kedua, status pembayaran biaya kuliah, serta jumlah mata kuliah yang diambil dan lulus di semester pertama dan kedua."""

print("\n Top 5 factors predicting student outcomes:")
for i, (feature, importance) in enumerate(zip(feat_importances['Feature'].head(5), feat_importances['Importance'].head(5))):
    print(f"{i+1}. {feature}: {importance:.4f}")

"""Plotting untuk menampilkan fitur-fitur lainnya"""

# Visualisasi top 15 fitur
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feat_importances.head(15))
plt.title('Top 15 Factors Influencing Student Success', fontsize=16)
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.tight_layout()
plt.show()

"""# **EVALUATION & COMPARATION**

Empat model klasifikasi dilatih dan dievaluasi untuk membandingkan performa prediksi hasil mahasiswa, dengan pendekatan ensemble (XGBoost dan Random Forest), model linier (Logistic Regression), dan instance-based (KNN).
"""

models = {
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "KNN": KNeighborsClassifier(),
}

# List for storing accuracy and classification report
accuracies = []
reports = []

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train_res, y_train_res)  # Train model
    y_pred = model.predict(X_test)  # Predict on test set
    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
    accuracies.append(acc)
    reports.append(classification_report(y_test, y_pred, output_dict=True))  # Store detailed report

"""XGBoost memberikan hasil prediksi terbaik secara keseluruhan, terutama pada kelas mayoritas, diikuti oleh Random Forest dan Logistic Regression. KNN menunjukkan performa paling rendah dan kurang efektif untuk digunakan pada kasus ini."""

# Plotting the accuracy for each model
plt.figure(figsize=(10, 6))
plt.bar(models.keys(), accuracies, color=['blue', 'green', 'red', 'orange'])
plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.show()

# Display classification reports for each model
for name, report in zip(models.keys(), reports):
    print(f"\n{name} Classification Report:")
    print("--------------------------------------------------")
    print(report)